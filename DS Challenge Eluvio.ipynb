{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM STATEMENT: What Year is it?\n",
    "\n",
    "## Using NLP techniques on date_created and title, predict the probability of when an event occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/quentinpompliano/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/quentinpompliano/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_created</th>\n",
       "      <th>date_created</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>title</th>\n",
       "      <th>over_18</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1201232046</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Scores killed in Pakistan clashes</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1201232075</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Japan resumes refuelling mission</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1201232523</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>US presses Egypt on Gaza border</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1201233290</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Jump-start economy: Give health care to all</td>\n",
       "      <td>False</td>\n",
       "      <td>fadi420</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1201274720</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Council of Europe bashes EU&amp;UN terror blacklist</td>\n",
       "      <td>False</td>\n",
       "      <td>mhermans</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509231</th>\n",
       "      <td>1479816764</td>\n",
       "      <td>2016-11-22</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Heil Trump : Donald Trump s  alt-right  white...</td>\n",
       "      <td>False</td>\n",
       "      <td>nonamenoglory</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509232</th>\n",
       "      <td>1479816772</td>\n",
       "      <td>2016-11-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>There are people speculating that this could b...</td>\n",
       "      <td>False</td>\n",
       "      <td>SummerRay</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509233</th>\n",
       "      <td>1479817056</td>\n",
       "      <td>2016-11-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Professor receives Arab Researchers Award</td>\n",
       "      <td>False</td>\n",
       "      <td>AUSharjah</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509234</th>\n",
       "      <td>1479817157</td>\n",
       "      <td>2016-11-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Nigel Farage attacks response to Trump ambassa...</td>\n",
       "      <td>False</td>\n",
       "      <td>smilyflower</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509235</th>\n",
       "      <td>1479817346</td>\n",
       "      <td>2016-11-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Palestinian wielding knife shot dead in West B...</td>\n",
       "      <td>False</td>\n",
       "      <td>superislam</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509236 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_created date_created  up_votes  down_votes  \\\n",
       "0         1201232046   2008-01-25         3           0   \n",
       "1         1201232075   2008-01-25         2           0   \n",
       "2         1201232523   2008-01-25         3           0   \n",
       "3         1201233290   2008-01-25         1           0   \n",
       "4         1201274720   2008-01-25         4           0   \n",
       "...              ...          ...       ...         ...   \n",
       "509231    1479816764   2016-11-22         5           0   \n",
       "509232    1479816772   2016-11-22         1           0   \n",
       "509233    1479817056   2016-11-22         1           0   \n",
       "509234    1479817157   2016-11-22         1           0   \n",
       "509235    1479817346   2016-11-22         1           0   \n",
       "\n",
       "                                                    title  over_18  \\\n",
       "0                       Scores killed in Pakistan clashes    False   \n",
       "1                        Japan resumes refuelling mission    False   \n",
       "2                         US presses Egypt on Gaza border    False   \n",
       "3            Jump-start economy: Give health care to all     False   \n",
       "4         Council of Europe bashes EU&UN terror blacklist    False   \n",
       "...                                                   ...      ...   \n",
       "509231   Heil Trump : Donald Trump s  alt-right  white...    False   \n",
       "509232  There are people speculating that this could b...    False   \n",
       "509233          Professor receives Arab Researchers Award    False   \n",
       "509234  Nigel Farage attacks response to Trump ambassa...    False   \n",
       "509235  Palestinian wielding knife shot dead in West B...    False   \n",
       "\n",
       "               author   category  \n",
       "0               polar  worldnews  \n",
       "1               polar  worldnews  \n",
       "2               polar  worldnews  \n",
       "3             fadi420  worldnews  \n",
       "4            mhermans  worldnews  \n",
       "...               ...        ...  \n",
       "509231  nonamenoglory  worldnews  \n",
       "509232      SummerRay  worldnews  \n",
       "509233      AUSharjah  worldnews  \n",
       "509234    smilyflower  worldnews  \n",
       "509235     superislam  worldnews  \n",
       "\n",
       "[509236 rows x 8 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "df = pd.read_csv('Eluvio_DS_Challenge.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scores killed in pakistan clashes</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>japan resumes refuelling mission</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us presses egypt on gaza border</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jump-start economy: give health care to all</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>council of europe bashes eu&amp;un terror blacklist</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509231</th>\n",
       "      <td>heil trump : donald trump s  alt-right  white...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509232</th>\n",
       "      <td>there are people speculating that this could b...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509233</th>\n",
       "      <td>professor receives arab researchers award</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509234</th>\n",
       "      <td>nigel farage attacks response to trump ambassa...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509235</th>\n",
       "      <td>palestinian wielding knife shot dead in west b...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509236 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       preprocessed_title  year\n",
       "0                       scores killed in pakistan clashes  2008\n",
       "1                        japan resumes refuelling mission  2008\n",
       "2                         us presses egypt on gaza border  2008\n",
       "3            jump-start economy: give health care to all   2008\n",
       "4         council of europe bashes eu&un terror blacklist  2008\n",
       "...                                                   ...   ...\n",
       "509231   heil trump : donald trump s  alt-right  white...  2016\n",
       "509232  there are people speculating that this could b...  2016\n",
       "509233          professor receives arab researchers award  2016\n",
       "509234  nigel farage attacks response to trump ambassa...  2016\n",
       "509235  palestinian wielding knife shot dead in west b...  2016\n",
       "\n",
       "[509236 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def data_preprocessing(title): \n",
    "    title = title.lower()\n",
    "    title = [lemmatizer.lemmatize(word) for word in title]\n",
    "    title = ''.join(title)\n",
    "    \n",
    "    return title\n",
    "\n",
    "def get_year_preprocessing(date):\n",
    "    date = date[0:4]\n",
    "    date = int(date)\n",
    "    return date\n",
    "\n",
    "df['preprocessed_title'] = df['title'].apply(lambda title: data_preprocessing(title))\n",
    "df['year'] = df['date_created'].apply(lambda date: get_year_preprocessing(date))\n",
    "\n",
    "new_df = df[['preprocessed_title', 'year']].copy()\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (356465, 1) (356465,)\n",
      "Test data: (152771, 1) (152771,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = new_df.copy()\n",
    "y = data['year'].values\n",
    "data.drop(['year'], axis=1, inplace=True) # data now only contains preprocessed_title\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size = 0.3, stratify=y)\n",
    "                                                   #(preprocessed_title, year, %30 test size)\n",
    "print(\"Train data:\", X_train.shape, y_train.shape)\n",
    "print(\"Test data:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent occuring n-grams in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prime minister', 3050),\n",
       " ('north korea', 2826),\n",
       " ('year old', 2635),\n",
       " ('islamic state', 2592),\n",
       " ('human rights', 2211),\n",
       " ('saudi arabia', 1396),\n",
       " ('united states', 1388),\n",
       " ('al qaeda', 1364),\n",
       " ('climate change', 1226),\n",
       " ('south korea', 1122),\n",
       " ('south china', 1084),\n",
       " ('middle east', 971),\n",
       " ('west bank', 947),\n",
       " ('hong kong', 912),\n",
       " ('china sea', 887),\n",
       " ('foreign minister', 852),\n",
       " ('boko haram', 835),\n",
       " ('death toll', 826),\n",
       " ('north korean', 807),\n",
       " ('chemical weapons', 795)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(2,5), stop_words='english')\n",
    "#summaries = ''.join(new_df['preprocessed_title'])\n",
    "summaries = ''.join(X_train['preprocessed_title'])\n",
    "ngram_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "Counter(ngram_summaries).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize using TF-IDF Vectorizer and create X matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356465, 2000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "title_text = X_train['preprocessed_title'].values\n",
    "\n",
    "tfv = TfidfVectorizer(ngram_range=(2,5), max_features=2000)\n",
    "X = tfv.fit_transform(title_text).todense()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016 2015 2014 ... 2014 2016 2015]\n"
     ]
    }
   ],
   "source": [
    "y = pd.Series(y_train).values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Testing (using newton-cg optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prob_2008</th>\n",
       "      <th>Prob_2009</th>\n",
       "      <th>Prob_2010</th>\n",
       "      <th>Prob_2011</th>\n",
       "      <th>Prob_2012</th>\n",
       "      <th>Prob_2013</th>\n",
       "      <th>Prob_2014</th>\n",
       "      <th>Prob_2015</th>\n",
       "      <th>Prob_2016</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.231237</td>\n",
       "      <td>0.209235</td>\n",
       "      <td>0.030093</td>\n",
       "      <td>0.006137</td>\n",
       "      <td>0.046844</td>\n",
       "      <td>0.073055</td>\n",
       "      <td>0.126996</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>0.171003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.840940</td>\n",
       "      <td>0.155271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.055942</td>\n",
       "      <td>0.086314</td>\n",
       "      <td>0.077520</td>\n",
       "      <td>0.391085</td>\n",
       "      <td>0.100468</td>\n",
       "      <td>0.067164</td>\n",
       "      <td>0.061979</td>\n",
       "      <td>0.080097</td>\n",
       "      <td>0.079432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.065919</td>\n",
       "      <td>0.010625</td>\n",
       "      <td>0.006013</td>\n",
       "      <td>0.106739</td>\n",
       "      <td>0.547792</td>\n",
       "      <td>0.202974</td>\n",
       "      <td>0.051062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042087</td>\n",
       "      <td>0.022721</td>\n",
       "      <td>0.030087</td>\n",
       "      <td>0.097101</td>\n",
       "      <td>0.128286</td>\n",
       "      <td>0.279229</td>\n",
       "      <td>0.154225</td>\n",
       "      <td>0.128696</td>\n",
       "      <td>0.117568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.769732</td>\n",
       "      <td>0.160108</td>\n",
       "      <td>0.048029</td>\n",
       "      <td>0.018368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.062876</td>\n",
       "      <td>0.057577</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.072942</td>\n",
       "      <td>0.076312</td>\n",
       "      <td>0.168715</td>\n",
       "      <td>0.176515</td>\n",
       "      <td>0.178630</td>\n",
       "      <td>0.165169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>0.053552</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.841663</td>\n",
       "      <td>0.071425</td>\n",
       "      <td>0.018406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prob_2008  Prob_2009  Prob_2010  Prob_2011  Prob_2012  Prob_2013  \\\n",
       "0   0.231237   0.209235   0.030093   0.006137   0.046844   0.073055   \n",
       "1   0.000651   0.000570   0.000406   0.000527   0.000508   0.000577   \n",
       "2   0.055942   0.086314   0.077520   0.391085   0.100468   0.067164   \n",
       "3   0.002493   0.006383   0.065919   0.010625   0.006013   0.106739   \n",
       "4   0.042087   0.022721   0.030087   0.097101   0.128286   0.279229   \n",
       "5   0.001034   0.000784   0.000533   0.000738   0.000674   0.769732   \n",
       "6   0.062876   0.057577   0.041263   0.072942   0.076312   0.168715   \n",
       "7   0.003382   0.002936   0.002104   0.002808   0.053552   0.003724   \n",
       "\n",
       "   Prob_2014  Prob_2015  Prob_2016  \n",
       "0   0.126996   0.105400   0.171003  \n",
       "1   0.000551   0.840940   0.155271  \n",
       "2   0.061979   0.080097   0.079432  \n",
       "3   0.547792   0.202974   0.051062  \n",
       "4   0.154225   0.128696   0.117568  \n",
       "5   0.160108   0.048029   0.018368  \n",
       "6   0.176515   0.178630   0.165169  \n",
       "7   0.841663   0.071425   0.018406  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = LogisticRegression(solver='newton-cg')\n",
    "estimator.fit(X,y)\n",
    "\n",
    "source_test = [\"barack obama elected\", \"paris attacks\", \"osama bin laden\", \n",
    "               \"world cup\", \"lybian civil war\", \"edward snowden\", \"bitcoin surge\", \"ebola outbreak\"]\n",
    "\n",
    "Xtest = tfv.transform(source_test)\n",
    "pd.DataFrame(estimator.predict_proba(Xtest), columns=[\"Prob_2008\", \"Prob_2009\", \"Prob_2010\", \"Prob_2011\", \"Prob_2012\",\n",
    "                                                     \"Prob_2013\", \"Prob_2014\", \"Prob_2015\", \"Prob_2016\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    ">From the analysis above, we can make the following insights:\n",
    ">> 1. Check the probability trends to see when news goes \"out of date\"\n",
    ">> 2. Predict societal uptrends and downtrends based on news headlines (market, fads, etc.)\n",
    ">> 3. Predict the likelihood of an event occuring in a certain year\n",
    "\n",
    ">Improvements to make:\n",
    ">> 1. If LR model cannot make a prediction, it seems to assign generic probabilities (6, 7, 8, 9)\n",
    ">>> Possible solutions: \n",
    ">>>> Tune hyperparameters of model,\n",
    ">>>> take a larger subset of the data\n",
    ">>>> change ML model (SVM, KNN, or LVQ may work better)\n",
    ">> 2. Runtime for LR model takes about 10 minutes\n",
    ">>> Possible solutions:\n",
    ">>>> Tune hyperparameters,\n",
    ">>>> find a faster optimizer,\n",
    ">>>> take a smaller subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
